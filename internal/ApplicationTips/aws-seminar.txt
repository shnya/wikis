|| '''セミナー名''' || AWS EMR/Hadoopを使ったBig Data活用法 - エンタープライズ編 - ||
|| '''場所''' || AWS目黒オフィス (東京都目黒区下目黒1-8-1 アルコタワーアネックス（受付4階）)||
|| '''時間''' || 2012年05月17日(14:00-17:00) ||
|| '''告知URL''' || http://kokucheese.com/event/index/34636 ||

<<TableOfContents()>>

= Bigdata at Amazon =

 * 講師
 Adam Gray  Amazonシニアプロダクトマネージャ

== ビッグデータ分析の事例紹介 ==

農産物の収穫量の予測にビッグデータは利用されている。<<BR>>
気候データや、レーダーデータを活用し、洗練されたアルゴリズムではなく、ただ単純に大量のデータを利用することで、
非常に精度の高い解析を行う事例があった。

== ビッグデータの解析に重要な３つのこと ==

Volume データの量がどの程度あるかということ<<BR>>
Velocity どれだけ早くデータから情報を引き出すか、解析できるかということ<<BR>>
Variety データの形式がいろいろあるということ。Facebook, twitter, RDBなど、さまざまな形式のデータ<<BR>>

== Collect, Store, Analyze, Share ==

どのようにして、このようなデータを収集(Collect)、貯蔵(Store)、解析(Analyze)し、共有(Share)するかということをみていく。

=== Collect ===
データ収集の手段には下記のような方法がある。

S3 Gateway: <<BR>>
   オンプレミスのアプライアンスがある Offsite backup Data mirroring

Direct Connect: <<BR>>
  ローコストで高い帯域で、直接Amazon Public Cloudに接続する

Stream Dataの解析: <<BR>>
  オープンソースのツールなどが利用可能

=== Store ===
ここ２年でオプションがたくさん増加している大変興味深い分野。<<BR>>
構造とサイズ、アクセスパターンでデータの保存方法はジャンル分けが可能。

Amazon S3: <<BR>>
  クラウドベースのオブジェクトストア。<<BR>>
  大変大規模な非構造化データを対象としている。ファイル、画像など。

Dynamo DB: <<BR>>
  小規模なオブジェクトをローレイテンシで行うものに適している。

RDB: <<BR>>
  構造化されたデータに限定されてしまう。

=== Analyze ===
どのように情報を整理、クレンジングし、解析し、集約(aggregate)するかということ。

{{{
データ分析の例)
顧客のトランザクションデータ(Dynamo DB)と顧客データ(RDB)がある。
クリックストリームデータ(顧客のインタラクションのデータ),Third partyデータ(Facebook twitter)も収集したい。
このようなデータを集約して、分析することにより、何らかの日次レポート、月次レポートを作成したい。
}}}

このような異なるフォーマットのデータに対して、Hadoopを利用すれば、さまざまなスキーマのデータを利用するのに対して活用できる。

今までは、少数の大規模なテクノロジーカンパニーだけがこのように大規模なデータを利用できていた。<<BR>>
そういった状況の中で、Hadoopという分散処理の仕組みができたことで、敷居がかなり下がった。

もう一つのパラダイムシフトとして、クラウドとHadoopを組み合わせることにより、どのような企業でも大規模なデータを効率よく処理できるインフラができた。

この２つの仕組みにより大規模データ処理が大衆化した。

==== EMRの紹介 ====

Hadoopはまだできたばかりの仕組みであり、管理も複雑。<<BR>>
EMRはこの複雑性を緩和する仕組みであると言える。

例えばJobを50のマシンに分散する場合でも、簡単に実行可能。<<BR>>
また、EMRはAmazonのほかのインフラとシームレスに接続することができる。<<BR>>
例えば、HadoopはS3をファイルシステムとして利用することができる。

==== Hadoopのユースケース ====
１つ目はデジタル広告。顧客の利用状況を確認して、ターゲティングすることができる。<<BR>>
また、Hadoopを使うことにより大規模にログ、WEBの解析することができるようになりました。<<BR>>
さらにHadoopはData Ware Hauseの代わりとして利用することが可能。<<BR>>


多くの企業がHadoopを利用するようになった。<<BR>>
昨年AmazonはNo.1のHadoopソリューション会社として認定された。<<BR>>
認定された理由は、フィーチャーの多さが１つ。もう一つは、コスト効率の高さ。

==== EMRと連携できるフィーチャー ====

EMRでは、S3をファイルシステムとして利用した方がよい。<<BR>>
オンプレミスのHadoopクラスタとして活用する場合を考えてみた時、どの程度サーバが必要かを考えて、余剰に買ってしまい、コストを無駄にすることがある。<<BR>>
逆に、キャパシティが足りず問題になることがある。<<BR>>
S3を使えばこういったキャパシティプランニングの問題を解決できる。

==== Amazon Crowd Watch ====
EMRと連携可能で、ジョブの進捗状況や、クラスタの健全性、リソースの競合を確認することができる。

==== Cloud Watch Alarm ====
例えば独自のHadopのオートスケールを実現することができる。<<BR>>
例えば、HDFSが80%を超えると、アラートを出して、自動的に追加容量の拡張をすることができる。<<BR>>
タスクの数がノードの数に比べて多すぎる場合、自動的にノード数を追加することもできる。<<BR>>
逆にタスクの数が少なくなってきた場合、ノードの数を自動的に少なくすることもでき、必要に合わせて自動的にノードが縮小拡大可能である。


==== Web-Based Debugging ====
ログをWebから確認し、容易に障害や問題を把握することができる。

==== BigData Ecosystemとの連携 ====
BIシステム, Data Integration, Analytics, Visualizationといった分野でパートナーと連携している。<<BR>>
MicroStrategy(BI system)<<BR>>
Informatica(科学的なデータを扱えるようになった)<<BR>>
Karmasphere(ビジュアライズ, EMR上で１時間あたりのデータ分析など)


=== Share ===
分析後のデータをどのように共有するか。<<BR>>
visualize, explore, decideの３つの分野。<<BR>>
ビッグデータで興味深い分野だが、まだまだ革新の余地がある分野。<<BR>>
まだまだ成功したソリューションは少ない。

==== 事例 ====
 * Webのユーザを、時間、地域ごとに可視化した例。
 * LinkedInのグラフ構造を可視化。


== Amazon内部での利用例 ==

AmazonではAWSをどのように利用しているかという話<<BR>>
よく聞かれる質問は、「amazon.comはAWSを使っているか？」ということ。

実際にAWSは使っているが、どのようにAWSを使っているかを各チームに聞いてみた。

==== なぜAmazonでEMRが使われるようになったかについて ====

分散コンピューティングの新しい経済性を見いだすことができている。<<BR>>
データの解析には、革新的なパラダイムシフトがあり、より安価に大量のデータの解析が可能になった。<<BR>>
また、コスト削減も重要なことだが、それだけではなく、早いスピードでの解析は、早いスピードでイノベーションを起こすことができるということでもある。

=== ケーススタディ1 Amazonアソシエートプログラム ===
Amazonアソシエートプログラムは、AmazonのサードパーティのWebサイト上に広告をのせる仕組み。<<BR>>
写真も含めた商品の詳細な情報であったり、単純にAmazonへのリンクと画像を表示する広告もある。

アソシエートの計算(広告主にいくら払えばいいのかということ)が非常に大変だった。

==== 仕組みの説明 ====
Amazonアソシエートの広告主にいくら払えばいいかという計算は下記のようになっていた。
{{{
オーダー => C++アプリ(1) => 時間単位のファイル => C++アプリ(2) => 日次ファイル => C++アプリ(3) => ペイメントサービスに送信
}}}

一番の問題は、C++アプリ(2)（時間単位のファイルから日次ファイルの出力を行う計算)がスケールせずに大変だった。<<BR>>
特にホリデーシーズンの際に、大量のオーダーがあり、大変。<<BR>>
来年は第４四半期を再度乗り切ることができるかどうか不明な状況だった。

ここでの問題は、分散コンピューティングの難しさ。<<BR>>
1台でも増えれば、この問題に直面する。<<BR>>
横軸、サーバの台数、縦軸を困難さとしてプロットすると、2台目から困難さは1 => 10^6になり、そこからは台数を増やしてもあまり変わらない。

amazon.comでは分散コンピューティングの難しさに直面し、神レベルのエンジニアがいないと無理だということが判明した。

それを解決する１つ目のイノベーションはHadoopだった。

==== MapRecuceの説明 =====
ある数人の人たちがタスクを実行する場合を考える。ある一人の人間がタスクを実行するのにどれだけの時間がかかったかを考える。それぞれのタスクでの時間はMapperで時間を求めることができる。Reducerで各ユーザごとの合計時間を求める。

==== 1つ目のイノベーション Hadoop ====
Hadoopは実装はオープンソースで拡張性が高く、冗長性が高い分散システム。<<BR>>
今まで神レベルのエンジニアが必要だったが、今では良いエンジニアがいればよくなった。

{{{
Hadoopを使った場合のアソシエートの計算:
オーダー => フィルタプログラム => S3 => Hadoopクラスタで各広告主に計算
}}}

今まではデータが増えるにつれてさらに良いエンジニアが必要だったが、今はマシンが増えればよくなった。<<BR>>
Hadoopは分散システムを作る上でのコストを下げることができる。

==== ２つ目のイノベーション Amazon Cloud ====
２つ目は、分散システムの運用をすること。ここがコストが高くなっている。<<BR>>
(11月のAmazonトラフィックの例を見ながら)Amazonは季節変動が高いサービス。<<BR>>
Amazonの中では、ピーク値を計算する優秀なエンジニアが３人いた。<<BR>>
彼らはピーク値から15%増加した値を元に計算した。

しかし、この計算でサーバを購入した場合、実際は75%の時間は有効に使われていないことがわかった。<<BR>>
(たいていの時期はピークではないため)

Hadoopクラスタでもこの問題は同様に発生する。<<BR>>
Hadoopクラスタはオンプレミスで買った場合は、それほど利用されていない時間が多い。

しかし、今ではAmazon EC2が利用できるようになった。<<BR>>
Amazon EC2は台数を増やしたり、減らしたり、キャパシティを変更できる仕組み。

EMRは EC2とHadoopを組み合わせたもの。<<BR>>
Amazon EC2を使うことによって、利用されていない75%にコストを払わずにすむようになった。<<BR>>

また、Amazon EC2を使うことによりオペレーションコストも下げることができるようになった。<<BR>>
具体的には、サーバの構築コストも下げることができた。

=== ケーススタディ2 商品の属性決定 ===
次のケーススタディは、Amazonの属性をどのように決めるかという問題。<<BR>>
Amazonは1500億の商品を扱っている。<<BR>>
さまざまな商品属性を決めていく必要がある。(例えば関税の計算)<<BR>>

関税は国によって商品によって、全然異なる。<<BR>>
例えば、ゴルフシューズが典型的な例。衣料なのか、スポーツ製品なのかで関税が変わることもある。

関税だけでなく、危険物かどうかも重要な属性。航空便として配送できるかどうかに関わる。<<BR>>

ハイリスクかつ商品価値が高いという属性もある。iPhoneなど。商品価値が高く盗まれやすい小さいもの。<<BR>>
こういったものは、倉庫の特別な場所に保存されている。

Hadoopの利用前は手動で行っていた。<<BR>>
倉庫管理者は、商品カタログをエンジニアに渡し、エンジニアは、Data Ware HouseからSQL をたたき、HRVリスト?を作成する。HRVリストを管理者が確認するという流れ。

この仕組みはスケールしない。<<BR>>
なぜスケールしないかというと、データウェアハウスを使っているから。

私の好きな言葉<<BR>>
「データウェアハウスは良いデータが死ぬ場所」<<BR>>
理由は、保存するときは拡張性がない状態で格納しなければならず、一度保存したら取り出しにくいから。

現在は下記の仕組みでうまく動いている<<BR>>
商品カタログをS3上に保存(毎週5000万のアップデートがされる) => 1.5ビリオンのアイテムをEMRクラスタで計算。

=== ケーススタディ3 Cloud Drive ===
2012年3月に始めたオンラインストレージサービス。<<BR>>
新しい機能は適宜追加されていて、例えば、クラウドドライブ上にある音楽については、クラウドプレーヤーで再生できる機能がある。<<BR>>
この機能の追加は、ユーザのトレンドを元に決定された。

取得されたユーザ情報のログはS3にアップロードされ、EMRのクラスタを使って、ユーザ情報のトレンドを分析している。

最初は規模的に分散型の処理にする必要はないが、分散した処理にすることは簡単。<<BR>>
実際ユーザ情報の分析は、3000行のコードで実装された。<<BR>>
何が大事かというと、今後どれだけ規模が増えたとしても3000行のコードで済むということ。

Cloud Driveのエンジニアはこう言った。<<BR>>
「ログの処理は私の仕事ではない」

分散コンピューティングはそもそも難しいことだが、それが差別化できる要因ではないので、<<BR>>
その部分をアウトソーシングして差別化できる部分に注力するのは、非常にいいアイディア。

=== 最初の質問　===

最初の質問に戻ると、AmazonはAWSの製品を使っているか => 使っている。

内部でのEMRの利用数はうなぎ上りで増えている。<<BR>>
コストを下げてすぐ使えるようにしたことで、非常に安く実験ができるようになった。

安ければ安い程頻繁に使われるようになり、さらにイノベーションが促進されるようになった。

== 質疑応答 ==

=== HDFSとS3の使い分け ===
HDFSはSPoFだが、S3は耐久性の高いシステム。<<BR>>
使い分けとして、S3はバックアップとして使い、何度も実行されるようなデータだけHDFS上に置く。

=== namenodeのSPoF対策 ===
開発版ではNFSを使ったnamenode-HAの仕組みがあるが、まだ脆弱なものとしてとらえており、Amazon社内でもこの部分の改善案は考えている。<<BR>>
DynamoDBが使えないかと考えている。


=== 物流部分で障害があった場合に営業に支障が出ないのか ===
５万から100万のジョブが動いており、そのうちHWでの障害は非常に少ないため、SLAを保てる。<<BR>>
amazon.comの中にEMRのパフォーマンス向上のための対応チームがあるかどうかというと、完全にEMRのチームにアウトソースしている。<<BR>>
ここで重要なのは、amazon.comということで特別な対応はしていないということ。<<BR>>
ほかの顧客と同様にEMRのチームはパフォーマンス向上などの対応をしている。

=== 50TBのデータをダイナミックに解析を行うような場合、毎回JavaのMapReduceプログラムを書かないといけない? ===
GreenplumとかTerradataとかを使って分析することが多い?<<BR>>
AmazonではGreenplumなどの置き換えとなるソリューションも検討しているが、まだ詳細は言えない。

=== Amazon EC2は他のユーザのプロセスのせいで遅くなったりするが、その影響をEMRは受けるか？ ===
基本的には、その心配はない。<<BR>>
どうしても心配であれば、インスタンスサイズが大きくなればなるほど、共有するノードは少なくなるため、その心配は少なくなるはず。<<BR>>
また、クラスタコンピューティングでは、完全にサーバを占有することになるので、そのソリューションを利用するのも手である。


= Amazon Elastic MapReduce =

 * 講師
  松尾康博 ソリューションアーキテクト<<BR>>
  大谷と二人でビッグデータのソリューションに対して対応している。

== ここ半年のEMRのアップデート ==

 * 1月にHadoop 0.20.205をサポート(前までは0.18)
 　* 現状考えうる最も安定したHadoop

 * AMIバージョニング機能
   * 古いバージョンのHadoopを使い続けることも可能
   * AMI 1.0 旧来のHadoop 0.18/0.20.2
   * AMI 2.0.x 
     * Hadoop 0.20.205
     * Pig 0.9.2, Hive 0.7.1.4

 * S3DistCp
   * S3とHadoopの間でのデータの移動を最適化したツール。Apache DistCPをS3用にしたものと考えてくれれば良い。

 * VPC対応
   * VPC(仮想ネットワーク構築サービス)でEMRが稼働可能に。
     * ユースケース
       * 機密情報の処理
       * DirectConnect(専用線)でデータを転送

 * Amazon VPCの説明
   * EC2内に分離したサブネットを自由に作成
     * (内部でプライベートとパブリックを分けて完全に外部からアクセスしないようなネットワークを作成することも可能)
   * AWSクラウド上にプライベートクラウドを構築
     * 仮想ネットワーキング
     * VPN接続して閉域網でAWSにアクセス
     * オンプレミスとのハイブリッドが可能
     * 社内インフラの延長にAWSを拡張

 * AWS Direct Connect
   * 品川のデータセンター(エクイニクス相互接続ポイント)から専用線を引き込むことができる。
   * 1GB, 10GBの専用線を複数利用可能
   * 専用線を使うことでデータの転送速度を保証できる。

 * EMR on VPC
   * 同じVPC内のインスタンス同士でしか通信できないようにしたり、オンプレミス環境との接続が容易にできたりする。

 * クラスタインスタンスタイプのサポート
   * US東海岸のみ
   * SandyBridgeのCPUを利用。占有になるため、他のユーザからの影響に悩まされることがない。

 * ブートストラップアクション:
   * 起動時にユーザがHadoopをカスタマイズすることが可能

 * その他
   * S3マルチアップロードによるアップロード時間の短縮
   * EMR価格値下げ
   * DynamoDB統合機能
   * CloudWatchメトリクス拡充(HDFSのIOやUsageも確認できるようになった)
   * IAM対応(権限を持った人だけがEMRを使うという認証機能)


== EMRの事例と利用パターン ==
国内の事例(Sonet)を紹介。

 * 広告配信ログの分析
   * 1日平均10GB, 年間3.65TB
   * 1年分5TBをS3にアップロードしてEMRで解析
   * オンプレミスでの試算: 初期費用だけで数千万
   * EMR+S3+SQSの価格 毎月50万(年間600万)
     * 20分の１以下の支出で実現
   * スポットインスタンス(余っているサーバを利用)を活用してアドホック分析
     * コストを50%削減

{{{
  分析システムの仕組み)
  Uploader(sonet) => AWS SQS(MQ) => Batch(sonet) => AWS EMR 
}}}


=== EMRデザインパターン ===
クラウド上の理想敵な大量データ処理モデル
   * 構造化データ 半構造化データ => S3 => Batch Tier(EMR) => Speed Tier(HBase, SimpleDB, RDBMS, MongoDB, Cassandra) => リアルタイム分析

==== ログアグリゲータ: ====
ログなどの小さなデータをS3に集約。

==== Puller ====
データベースからAmazon S3にアップロード。(MySQLだと)Slaveから、S3にアップロードがおすすめ。

==== 定期的な処理 ====
EMRを立ち上げっぱなしの場合は、HDFS上に保存して処理。(ただし、EMR上では消える可能性があるので、マスターはS3上に置く)

==== データをウェアハウスへ展開 ====
Amazon S3 => ジョブフロー => Amazon RDS(データウェアハウスとか)

== コスト効率のベストプラクティス ==

 * ジョブの実行中にノードを追加していくことが可能なので、パフォーマンスを必要とするときだけノードを追加。
{{{
例)
常時起動しっぱなしのEMRクラスタ(9ノード) => バッチ処理時だけ25ノード => バッチが終了したら9ノードに。
}}}

 * EMR + Spotインスタンス
  * アドホックな処理はスポットインスタンスを利用。使用していないEC2キャパシティを低価格で提供。
  * スポット価格は需要と供給に応じて、定期的に変動。
  * リージョンゾーンを指定して価格を指定 => スポット価格が指定価格を上回ればインスタンス終了
{{{
実際に試算した結果)
スポットなしのコスト 4台 * 14hour * $0.50 = $28
スポットありのコスト $21.75で時間の短縮50%, コスト削減効果:-22%

一概に言えないが、Spotインスタンスを使って一時的に多くのノードを使った方が安くなることがある。
}}}

== EMR都市伝説 ==
EMRでよく質問されることを都市伝説としてまとめてみた。

 * Q. 仮想化した環境がオーバーヘッドになるのでは？
 * A. 
   * 状況次第です。
   * 隣人の影響を受けやすい状況もあります。
     * ただし回避策あり(できるだけ大きいインスタンスを利用する)
   * ネットワーク障害も確かにあります。
   * 仮想化レイヤはディスクIOとネットワークIOに若干のオーバヘッド
   * 計測する内容を間違えてはいけません。トータルコストで考えてください。

<<BR>>
 * Q. 物理ハードウェア vs 仮想化
   * 物理ハードウェアの方が速い
 * A.
   * 台数を任意に増加させることができるので、それでカバー

<<BR>>

 * Q. オンプレミスHadoopの方が安い
 * A.
   * 物理サーバは最近本当に安い
   * Hadoopは高価なハードウェアは不要
   * でもNW機器のコストは別
   * ラック越えしたあたりからNW機器コストが高くなる。
   * EMRならばNWコストは不要
   * ハードウェア調達の時間的コストも別
   * 調達 - インストール - 設定する時間は無視できないコスト

<<BR>>

 * Q. HDFSは十分な堅牢性がありますよね？
 * A. 
  * まだまだ十分ではない
    * namenodeが落ちたらアウトだし
  * EMRはS3の堅牢性で低価格に解決


=== EMRを使う障壁 ===

 * S3へのデータアップロードと試行によるコストの発生が
   * AWS Import/Exportによるディスクそのものでアップロード
   * 専用線を引いていただくというやりかたで解決が可能
 * EMRとEC2はで実施したいハードウェア設定ができない
 * EMRでは使いたいHadoopディストリビューション(CDH)が使えない

<<BR>>

 * Q. AWSもリソースが足りなくなるのでは？
 * A. Amazon.comは2000年27.6億ドルの企業だった時に必要だったキャパシティを毎日追加しているので、心配しないでください。

<<BR>>

 * Q. Hadoopの面倒はみてくれないのでは？
   * CDHだとclouderaが面倒みてくれるけど。。
 * A. 
   * EMRのHadoopは深刻な問題に対してはパッチを適用
   * お客様のセキュリティポリシーが許せばお客様のクラスタにSSHでログインして問題解決にあたる。
   * (ただし、上記のサポートはAWSプレミアムサポートに入る必要がある)

== 結論 ==
EMRはビッグデータのためのキラーソリューション<<BR>>
大量ログ解析やバッチ処理のニーズは大きい<<BR>>
EMRはHadoopの煩雑さをカバーするサービス

